{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjIELFnOaDSx"
      },
      "source": [
        "# 2025-1 Artificial Intelligence (01)\n",
        "## Homework #3: EN-FR Machine Translation Using LSTM, Attention, and Transformer\n",
        "---\n",
        "Copyright (c) Prof. Jaehyeong Sim\n",
        "\n",
        "Department of Computer Science and Engineering\n",
        "\n",
        "College of Artificial Intelligence\n",
        "\n",
        "Ewha Womans University"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVZzUEccaIg1"
      },
      "source": [
        "## Guideline\n",
        "### Introduction\n",
        "*   Here in this homework, we will implement a **EN-FR machine translator** in PyTorch using three models: an **LSTM**, an **LSTM with Attention**, a **Transformer**.\n",
        "*   We didn't cover **NLP pipeline** in class, so the code might look complicated. I tried to explain the code as clearly as possible, and if you understand the entire code, you can now understand the basics of NLP pipeline and how the models work. So I **highly recommend you to read the code and the explanation carefully and understand them**.\n",
        "*   The training of each model takes long time (LSTM: 70 min, LSTM w/ attn: 120 min, Transformer: 50 min), so I suggest you start this homework early.\n",
        "\n",
        "### Your job\n",
        "1. Please complete the code. You only have to write the parts marked as **# TODO**.\n",
        "2. Please answer the discussion topics at the bottom of this notebook in a **separate PDF file**.\n",
        "\n",
        "### Submission guide\n",
        "1. Please rename the completed skeleton file to ***STUDENT_ID*.ipynb**. Your own student ID goes to *STUDENT_ID*. For example, if your student ID is 2512345, the file name should be **2512345.ipynb**. Also, make your PDF file name ***STUDENT_ID*.pdf***.\n",
        "2. Make sure that your notebook contains the **output of each cell** including the translation results with your own sentence.\n",
        "3. Turn in them into the Ewha CyberCampus.\n",
        "\n",
        "\n",
        "⚠ If you doesn't follow the submission guide above, you will get **5 point deduction** from this homework score.\n",
        "\n",
        "### Deadline\n",
        "*   **June 4, 23:59**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgaq-Hs1acBU"
      },
      "source": [
        "### 1. Necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -Uq \"datasets>=2.19\" \"fsspec>=2023.6.0\" sentencepiece sacrebleu\n",
        "%pip install torchinfo torch numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "36UvS6oXZpvK"
      },
      "outputs": [],
      "source": [
        "import math, random, pathlib, os, time, sentencepiece as spm\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrQWBZshasGx"
      },
      "source": [
        "### 2. Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vrQB72YFWsSD"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "vocab_size = 4000\n",
        "subset_size = 50000\n",
        "max_len = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lZ0tlO0RWsSD"
      },
      "outputs": [],
      "source": [
        "# LSTM\n",
        "lstm_epochs = 15\n",
        "lstm_layers = 3\n",
        "lstm_hidden = 1024\n",
        "lstm_batch_size = 128\n",
        "lstm_log_interval = 50\n",
        "lstm_dropout = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2yZFzAzsauMK"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "trans_base_lr = 5e-4\n",
        "d_model      = 512\n",
        "nhead        = 8\n",
        "nlayers      = 4\n",
        "ffn_dim      = 2048\n",
        "trans_epochs   = 15\n",
        "trans_log_interval = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3y2dU-7a8ak"
      },
      "source": [
        "### 3. Preprocessing\n",
        "Overall flow:\n",
        "\n",
        "\n",
        "```\n",
        "raw text (train.en/fr)\n",
        "↓\n",
        "learn vocab (SentencePieceTrainer)\n",
        "↓\n",
        "text to IDs (encode())\n",
        "↓\n",
        "model (LSTM, Transformer)\n",
        "```\n",
        "\n",
        "1.   Corpus acquisition\n",
        "  *   Goal: obtain a parallel English-French corpus.\n",
        "  *   load_dataset(): Hugging Face Datasets downloads the IWSLT 2017 TED-talk translations and returns a list of dictionaries\n",
        "  *   Having the two languages side by side is what later lets us train a sequence-to-sequence model.\n",
        "\n",
        "2.   Sampling a subset\n",
        "  *   The original machine-translation corpora is big. Shuffling with a fixed seed (42) and picking the first subset_size examples makes experiments reproducible and keeps training time reasonable for Colab GPU (T4).\n",
        "\n",
        "3. Writing raw text files (train.en, train.fr)\n",
        "  *   SentencePiece’s trainer expects one sentence per line of plain text. Saving the sample accomplishes two things:\n",
        "    * Gives SentencePiece its required input format.\n",
        "    * Lets you open the files in a text editor to see the real sentences the model will see.\n",
        "\n",
        "4. Learning a sub-word vocabulary with SentencePiece (BPE)\n",
        "  * What is vocabulary learning?\n",
        "    * Deep learning models consume numbers, not strings. “Vocabulary learning”  decides which text fragments become tokens and assigns each fragment a unique integer ID.\n",
        "    * Classic NLP used a fixed word list. Rare or misspelled words were pushed into a single \\<unk> bucket → information loss.\n",
        "      * \\<unk>: special token for any character sequence not in the learned vocab.\n",
        "    * Today we prefer sub-word units (e.g. Byte-Pair Encoding, WordPiece, Unigram). They split unseen words like:\n",
        "      \n",
        "      *internationalization* (not in vocab) → *international* ##*ization* (both are in vocab)\n",
        "\n",
        "      so the model still sees meaningful pieces and you keep vocabulary size manageable.\n",
        "  * Why sub-words instead of words?\n",
        "    * Open-vocabulary: can spell out the vocabulary it has never seen.\n",
        "    * Keeps vocab_size small so embedding matrices fit in memory.\n",
        "  * The trainer is configured with explicit IDs/pieces for \\<pad>, \\<unk>, \\<s> (BOS), \\</s> (EOS) because your downstream model will need to know exactly which integers correspond to padding, beginnig-of-sentence, etc. Changing them later would silently corrupt training.\n",
        "\n",
        "5. Runtime tokenizer setup\n",
        "  * A SentencePieceProcessor loads the freshly trained bpe.model and exposes:\n",
        "    * encode(str) -> List[int]\n",
        "    * special-token IDs (pad_id(), bos_id(), …)\n",
        "    * total vocabulary size (get_piece_size()).\n",
        "  * The utility encode() function truncates long sentences to max_len-1 tokens and appends an explicit EOS_ID. (RNNs/Transformers work best when they know where to stop decoding.)\n",
        "\n",
        "6. TranslationDataset\n",
        "  * A PyTorch Dataset that lazily keeps the raw strings. We postpone tokenization to the collate step so each mini-batch can be truncated/padded to its own maximum length—this is more memory-efficient than padding everything to a corpus-wide max.\n",
        "\n",
        "7. Mini-batch collation\n",
        "  * collate():\n",
        "    * Tokenizes every (src, tgt) pair with encode().\n",
        "    * Finds the longest sequence length inside that batch.\n",
        "    * Right-pads shorter sequences with \\<pad> so torch.tensor() can stack them into a rectangular batch_size × seq_len tensor.\n",
        "  * The result is two LongTensors ready for nn.Embedding → encoder/decoder → loss calculation.\n",
        "\n",
        "8. DataLoaders\n",
        "  * Train loader pulls 50 000 sentence pairs, shuffles each epoch, and applies our custom padding.\n",
        "  * Validation loader uses a fixed 1 000-sentence slice with deterministic order.\n",
        "  * Both loaders now stream GPU-ready batches you can feed directly into an LSTM or Transformer model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzycaZCFa9FT"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# 1.  Corpus acquisition\n",
        "# ---------------------------------------------------------------------------\n",
        "DATA_DIR = pathlib.Path('data') # Directory where all assets will live\n",
        "DATA_DIR.mkdir(exist_ok=True) # Safely create it the first time we run\n",
        "\n",
        "print(\"Downloading IWSLT 2017 EN-FR dataset …\")\n",
        "# `load_dataset` fetches a pre-tokenized parallel corpus of\n",
        "# English (\"en\") and French (\"fr\") sentences.  The corpus ships with\n",
        "# predefined splits (train / validation / test).\n",
        "ds = load_dataset('IWSLT/iwslt2017', # dataset identifier (repo_name/config)\n",
        "                  'iwslt2017-en-fr', # configuration: language pair\n",
        "                  split='train', # which split to load\n",
        "                  cache_dir=DATA_DIR, # store raw data under ./data\n",
        "                  trust_remote_code=True # allow community dataset scripts\n",
        "                  )\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2.  Sampling a manageable subset from the dataset to make training simpler\n",
        "# ---------------------------------------------------------------------------\n",
        "# Shuffling with a fixed seed ensures reproducibility: we always pick the\n",
        "# same sentences each run, making debugging easier.\n",
        "sampled = ds.shuffle(seed=42).select(range(subset_size))\n",
        "\n",
        "# Save raw text copies because SentencePiece expects plain‑text files for\n",
        "# training. These files are also handy for quick inspection with a text\n",
        "# editor.\n",
        "src_path = DATA_DIR/'train.en' # English sentences\n",
        "tgt_path = DATA_DIR/'train.fr' # French  sentences\n",
        "src_sentences = [ex['translation']['en'] for ex in sampled]\n",
        "tgt_sentences = [ex['translation']['fr'] for ex in sampled]\n",
        "src_path.write_text('\\n'.join(src_sentences), encoding='utf-8')\n",
        "tgt_path.write_text('\\n'.join(tgt_sentences), encoding='utf-8')\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3.  Sub‑word vocabulary learning with SentencePiece (BPE)\n",
        "# ---------------------------------------------------------------------------\n",
        "# Why sub‑word? It handles open vocabulary problems (e.g. new place names)\n",
        "# better than word‑level tokenizers while keeping sequence length reasonable.\n",
        "print(\"Training SentencePiece …\")\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=','.join([str(src_path), str(tgt_path)]), # both languages\n",
        "    model_prefix=str(DATA_DIR/'bpe'), # outputs bpe.model / bpe.vocab\n",
        "    vocab_size=vocab_size,\n",
        "    # Special tokens ─ IDs must match downstream model expectations.\n",
        "    pad_id=0,    pad_piece='<pad>',\n",
        "    unk_id=1,    unk_piece='<unk>',\n",
        "    bos_id=2,    bos_piece='<s>',\n",
        "    eos_id=3,    eos_piece='</s>',\n",
        "    character_coverage=0.9995, # keep almost every UTF‑8 char seen\n",
        "    model_type='bpe' # byte‑pair encoding variant\n",
        ")\n",
        "print(\"Done!\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4.  Runtime helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "# Choose CPU vs GPU automatically.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the learned sub‑word model to tokenize on‑the‑fly.\n",
        "sp = spm.SentencePieceProcessor(model_file='data/bpe.model')\n",
        "PAD_ID = sp.pad_id()\n",
        "BOS_ID = sp.bos_id()\n",
        "EOS_ID = sp.eos_id()\n",
        "VOCAB = sp.get_piece_size()\n",
        "\n",
        "# ---------------------------\n",
        "# Encoding utility\n",
        "# -------------------------\n",
        "def encode(sentence):\n",
        "    \"\"\"Convert raw text to a list of integer token IDs.\n",
        "\n",
        "    • Truncate to `max_len‑1` to leave room for the explicit EOS.\n",
        "    • Append EOS so the decoder knows where to stop.\n",
        "    \"\"\"\n",
        "    ids = sp.encode(sentence, out_type=int)[:max_len-1]\n",
        "    return ids + [EOS_ID]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5.  PyTorch Dataset wrapper\n",
        "# ---------------------------------------------------------------------------\n",
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Lazy wrapper that gives (src_sentence, tgt_sentence) tuples.\"\"\"\n",
        "    def __init__(self, split):\n",
        "        ds = load_dataset('IWSLT/iwslt2017', 'iwslt2017-en-fr', split=split)\n",
        "        self.src = [ex[\"translation\"][\"en\"] for ex in ds]\n",
        "        self.tgt = [ex[\"translation\"][\"fr\"] for ex in ds]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src[idx], self.tgt[idx]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6.  Batch collation: padding & tensor conversion\n",
        "# ---------------------------------------------------------------------------\n",
        "def collate(batch):\n",
        "    \"\"\"Custom collation to handle variable‑length sentences.\n",
        "\n",
        "    Steps\n",
        "    -----\n",
        "    1. Tokenize each sentence pair.\n",
        "    2. Compute max length inside the mini‑batch.\n",
        "    3. Right‑pad with <pad> so tensors become rectangular (B × T).\n",
        "    4. Return int64 tensors ready for `nn.Embedding` / `Transformer`.\n",
        "    \"\"\"\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_ids = [encode(s) for s in src_batch]\n",
        "    tgt_ids = [encode(t) for t in tgt_batch]\n",
        "    src_max = max(len(x) for x in src_ids)\n",
        "    tgt_max = max(len(y) for y in tgt_ids)\n",
        "    src_pad = [x + [PAD_ID]*(src_max-len(x)) for x in src_ids]\n",
        "    tgt_pad = [y + [PAD_ID]*(tgt_max-len(y)) for y in tgt_ids]\n",
        "\n",
        "    return torch.tensor(src_pad), torch.tensor(tgt_pad)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 7.  DataLoaders\n",
        "# ---------------------------------------------------------------------------\n",
        "train_loader = DataLoader(TranslationDataset('train[:50000]'), # subset for simpler experiments\n",
        "                          batch_size=lstm_batch_size,\n",
        "                          shuffle=True,\n",
        "                          collate_fn=collate # our custom padding logic\n",
        "                          )\n",
        "\n",
        "val_dataset = TranslationDataset(split=\"validation[:1000]\")\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=lstm_batch_size,\n",
        "                        shuffle=False, # deterministic validation order\n",
        "                        collate_fn=collate)\n",
        "\n",
        "# `train_loader` and `val_loader` now stream padded token‑ID tensors\n",
        "# that can be fed straight into an LSTM and Transformer encoder‑decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtuVuWwW_jdV"
      },
      "source": [
        "### 4-1. LSTM baseline: Class definition\n",
        "\n",
        "Here, you need to use two modules:\n",
        "\n",
        "*   nn.Embedding\n",
        "  *   Turns a batch of integer token IDs into a batch of dense vectors (embedded vectors).\n",
        "  *   Input: ints in [0, vocab_size-1]\n",
        "  *   Output: lookup of a trainable table with shape [vocab, hidden]\n",
        "*   nn.LSTM\n",
        "  *   Learns to compress a sequence of those vectors into hidden states that capture context.\n",
        "  *   Input shape must be [B, T, H] if batch_first=True.\n",
        "  *   Returns every hidden state plus the last hidden & cell states separately.\n",
        "\n",
        "Please refer to the official documentation of PyTorch for detailed usage of each module.\n",
        "You should be careful about tensor shapes. I encourage you to print out tensor shapes the first time they run a batch:\n",
        "\n",
        "\n",
        "```\n",
        "print(x.shape, emb.shape, outs.shape, h.shape)   # sanity check\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFu14pmr_vyh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, hidden):\n",
        "        super().__init__()\n",
        "        # ---- TODO students implement below ---- #\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ---- TODO students implement below ---- #\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, hidden):\n",
        "        super().__init__()\n",
        "        # ---- TODO students implement below ---- #\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, y, hidden):\n",
        "        # ---- TODO students implement below ---- #\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLSc1rZ8XtjX"
      },
      "source": [
        "### 4-2. LSTM baseline: Training loop\n",
        "\n",
        "**label_smoothed_nll_loss**\n",
        "\n",
        "In vanilla cross-entropy training the model is rewarded only when it places all probability mass on the single gold token.\n",
        "\n",
        "Side-effect: the network often becomes over-confident (p ≈ 1), which hurts generalization.\n",
        "\n",
        "Label smoothing fixes that by distributing a small portion ε of the probability mass over all classes.\n",
        "For a vocabulary of K tokens:\n",
        "\n",
        "```\n",
        "gold token prob      = 1 − ε\n",
        "every other token    = ε / K\n",
        "```\n",
        "We therefore minimize:\n",
        "\n",
        "```\n",
        "L = (1 − ε)⋅NLL + ε⋅UniformLoss\n",
        "```\n",
        "where\n",
        "* NLL = − log p(gold)\n",
        "* UniformLoss = − mean_j log p(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03AnyYufWsSE"
      },
      "outputs": [],
      "source": [
        "def label_smoothed_nll_loss(lprobs, # raw logits  (B, T, V)  OR (any, V)\n",
        "                            target, # gold token IDs (B, T)\n",
        "                            epsilon=0.1, # smoothing factor ε\n",
        "                            ignore_index=PAD_ID # which ID means \"padding\"?\n",
        "                            ):\n",
        "    \"\"\"\n",
        "    Cross-entropy with label smoothing, padding aware.\n",
        "    Returns: scalar mean loss over non-pad tokens.\n",
        "    \"\"\"\n",
        "    n_class = lprobs.size(-1) # V = vocabulary size\n",
        "\n",
        "    # 1) Convert logits → log-probabilities for numerical stability\n",
        "    lprobs = F.log_softmax(lprobs, dim=-1)\n",
        "\n",
        "    # 2) Flatten batch/time dims so every token is an independent row\n",
        "    lprobs = lprobs.view(-1, n_class)  # (B·T, V)\n",
        "    target = target.contiguous().view(-1) # (B·T,)\n",
        "\n",
        "    # 3) Build mask for <pad> tokens and neutralize them\n",
        "    pad_mask = target.eq(ignore_index) # True where token == PAD_ID\n",
        "    target   = target.masked_fill(pad_mask, 0)  # dummy index 0 won’t be used\n",
        "\n",
        "    # 4) Negative-log-likelihood of the gold token\n",
        "    nll_loss     = F.nll_loss(lprobs, target,\n",
        "                              reduction='none' # keep per-token loss (B·T,)\n",
        "                              )\n",
        "\n",
        "    # 5) “Uniform” loss term: −Σ_j log p(j) / V\n",
        "    smooth_loss  = -lprobs.sum(dim=-1) / n_class\n",
        "\n",
        "    # 6) Interpolate:    (1-ε)·NLL  +  ε·Uniform\n",
        "    loss = (1 - epsilon) * nll_loss + epsilon * smooth_loss\n",
        "\n",
        "    # 7) Remove padding from both numerator & denominator\n",
        "    loss.masked_fill_(pad_mask, 0.0) # zero where pad\n",
        "    return loss.sum() / (~pad_mask).sum() # average over real tokens\n",
        "\n",
        "# Convenience alias so the usual training loop can read:\n",
        "criterion = label_smoothed_nll_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtrRSbE2p4cT"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Model instantiation\n",
        "# ------------------------------\n",
        "# `VOCAB`, `lstm_hidden`, and `device` are defined earlier in the notebook.\n",
        "lstm_enc = Encoder(VOCAB, lstm_hidden).to(device)\n",
        "lstm_dec = Decoder(VOCAB, lstm_hidden).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7axG7LXXtI_"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# Training loop – Encoder–Decoder LSTM with label smoothing\n",
        "# ================================================================\n",
        "# This block shows one full experiment script: model instantiation, optimizer,\n",
        "# learning‑rate scheduler, epoch training, and validation evaluation.\n",
        "\n",
        "# ------------------------------\n",
        "# Optimizer\n",
        "# ------------------------------\n",
        "# Adam with the beta values (0.9, 0.98) and tiny eps for safety.\n",
        "optim = torch.optim.Adam(list(lstm_enc.parameters())+list(lstm_dec.parameters()),\n",
        "                          lr=1e-3, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# ------------------------------\n",
        "# LR scheduler: halve the LR every 5 epochs\n",
        "# ------------------------------\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.5)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# train_epoch() – one full sweep over the training DataLoader\n",
        "# ------------------------------------------------------------\n",
        "def train_epoch():\n",
        "    lstm_enc.train() # activate dropout & norm in train mode\n",
        "    lstm_dec.train()\n",
        "    total, n = 0, 0\n",
        "    for step, (src, tgt) in enumerate(train_loader, 1):\n",
        "        # Move mini‑batch to GPU/CPU device\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optim.zero_grad() # clear stale gradients\n",
        "\n",
        "        # Encoder forward pass\n",
        "        enc_out, hidden = lstm_enc(src) # hidden = (h_n, c_n)\n",
        "\n",
        "        # Decoder forward – feed gold tokens shifted right\n",
        "        logits, _ = lstm_dec(tgt[:, :-1], hidden)\n",
        "\n",
        "        # Flatten (B, T, V) → (B·T, V) and compute label‑smoothed CE\n",
        "        loss = criterion(logits.reshape(-1, VOCAB), tgt[:,1:].reshape(-1))\n",
        "\n",
        "        # Back‑prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to keep training stable (max‑norm = 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_enc.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(lstm_dec.parameters(), 1.0)\n",
        "\n",
        "        # Optimizer step (updates parameters)\n",
        "        optim.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        total += loss.item(); n += 1\n",
        "\n",
        "        # Report training loss every `lstm_log_interval` mini‑batches\n",
        "        if step % lstm_log_interval == 0 or step == 1:\n",
        "            print(f\"[batch {step:4}/{len(train_loader)}] \"\n",
        "            f\"loss={loss.item():.3f}\")\n",
        "\n",
        "    return total / n # epoch‑average loss\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# evaluate_loss() – no‑grad validation loop\n",
        "# ------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(model_enc, model_dec, loader, criterion, pad_id=PAD_ID):\n",
        "    model_enc.eval() # eval mode = disable dropout\n",
        "    model_dec.eval()\n",
        "    total, ntok = 0.0, 0 # token‑level aggregation\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        src_mask = (src != pad_id)\n",
        "\n",
        "        enc_out, hidden = model_enc(src)\n",
        "        logits, _ = model_dec(tgt[:, :-1], hidden)\n",
        "\n",
        "         # loss averaged per token (criterion already ignores PAD)\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            tgt[:, 1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        tokens = (tgt[:, 1:] != pad_id).sum().item() # non‑pad count\n",
        "        total += loss.item() * tokens # scale back to sum\n",
        "        ntok  += tokens\n",
        "\n",
        "    avg = total / ntok # mean NLL\n",
        "    ppl = math.exp(avg) # perplexity = e^(NLL)\n",
        "    return avg, ppl\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Main training loop across epochs\n",
        "# ------------------------------------------------------------\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "for epoch in range(1, lstm_epochs+1):\n",
        "    loss = train_epoch() # one pass over train set\n",
        "    val_loss, val_ppl = evaluate_loss(lstm_enc, lstm_dec, val_loader, criterion) # validation metrics\n",
        "    print(f\"Epoch {epoch}: Train loss={loss:.3f}, Val loss={val_loss:.3f}, Val ppl={val_ppl:.3f}\")\n",
        "\n",
        "    # Step the LR scheduler once per epoch\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "elapsed = time.perf_counter() - t0\n",
        "print(f\"[LSTM] Wall-clock training time : {elapsed/60:6.2f} min\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrv6_Xr6VmzM"
      },
      "source": [
        "### 5-1. Attention on LSTM: Class definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWC0sE6hVpQH"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# Scaled dot‑product Attention (single‑head, batched)\n",
        "# ------------------------------------------------------------------\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super().__init__()\n",
        "        # Linear layer projects decoder hidden → key/query space.\n",
        "        # Bias is set to False to keep the operation just a matrix mult.\n",
        "        self.W = nn.Linear(hidden, hidden, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_out):\n",
        "        \"\"\"Compute context vectors and attention weights.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        decoder_hidden : [B, 1, H]  – current decoder time‑step hidden state\n",
        "        encoder_out    : [B, T_src, H] – all encoder outputs (keys/values)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        context        : [B, 1, H]\n",
        "        attn_weights   : [B, 1, T_src]\n",
        "        \"\"\"\n",
        "\n",
        "        # ---- TODO students implement below ---- #\n",
        "        # 1. Project decoder hidden through self.W  →  [B, 1, H]\n",
        "        # 2. Dot‑product with encoder_out^T via torch.bmm\n",
        "        #      scores = Q · K^T  →  [B, 1, T_src]\n",
        "        # 3. Softmax over T_src dimension to turn scores → probs\n",
        "        # 4. Weighted sum (context) = probs · V  via torch.bmm again\n",
        "        # --------------------------------------- #\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Attention‑augmented Decoder (one token at a time)\n",
        "# ------------------------------------------------------------------\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, vocab, hidden):\n",
        "        super().__init__()\n",
        "        # ---- TODO students implement below ---- #\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, y, hidden, enc_outputs):\n",
        "        \"\"\"Args\n",
        "        y           : [B, T_dec]   – gold tokens\n",
        "        hidden      : (h, c) tuple each [L, B, H]\n",
        "        enc_outputs : [B, T_src, H]\n",
        "        Returns\n",
        "        -------\n",
        "        logits      : [B, T_dec, vocab]\n",
        "        new_hidden  : (h_n, c_n)\n",
        "        \"\"\"\n",
        "        # ---- TODO students implement below ---- #\n",
        "        # 1. Embed y and apply dropout → emb  [B, T_dec, H]\n",
        "        # 2. Loop over each time‑step t because\n",
        "        #    we want to feed the previous decoder hidden to attention.\n",
        "        # 3. For every t, do attention and lstm operation\n",
        "        # 4. Concatenate outputs → [B, T_dec, H]\n",
        "        # 5. Apply out_dp then fc → logits\n",
        "        # 6. Return logits and last hidden state tuple.\n",
        "        # --------------------------------------- #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7uLGGsXtSRz"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Model instantiation\n",
        "# ------------------------------\n",
        "attn_enc = Encoder(VOCAB, lstm_hidden).to(device)\n",
        "attn_dec = AttnDecoder(VOCAB, lstm_hidden).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT0xUCtKYKlU"
      },
      "source": [
        "### 5-2. Attention on LSTM: Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLgerpCMYMcj"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# Training loop – Attention‑based Sequence‑to‑Sequence Model\n",
        "# ================================================================\n",
        "# -------------------------------------------------------------------------------------------------\n",
        "# Legend\n",
        "#  • `CrossEntropyLoss`      – vanilla CE (label smoothing was already demonstrated earlier)\n",
        "#  • `ReduceLROnPlateau`     – scheduler that halves LR when validation loss stagnates\n",
        "# -------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ------------------------------\n",
        "# Optimizer setup\n",
        "# ------------------------------\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "optimizer = torch.optim.Adam(list(attn_enc.parameters()) + list(attn_dec.parameters()), lr=1e-3)\n",
        "\n",
        "# LR drops by ×0.5 if val‑loss fails to improve for one epoch.\n",
        "# `mode='min'` because we want the loss to go down.\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# train_epoch() – with attention\n",
        "# ------------------------------------------------------------\n",
        "def train_epoch():\n",
        "    attn_enc.train()\n",
        "    attn_dec.train()\n",
        "    total, ntok = 0, 0\n",
        "    for step, (src, tgt) in enumerate(train_loader, 1):\n",
        "       # ---------------- Mini‑batch prep ----------------\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ---------------- Forward pass -------------------\n",
        "        enc_out, hidden = attn_enc(src)\n",
        "        logits, _ = attn_dec(tgt[:, :-1], hidden, enc_out)\n",
        "\n",
        "        # CE expects (N, C) so reshape B×T×V → (B·T, V)\n",
        "        loss = criterion(logits.reshape(-1, VOCAB),\n",
        "                         tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        # ---------------- Back‑prop ----------------------\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(attn_enc.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(attn_dec.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # ---------------- Stats --------------------------\n",
        "        tokens = (tgt[:,1:] != PAD_ID).sum().item()\n",
        "        total += loss.item() * tokens # accumulate sum over tokens\n",
        "        ntok  += tokens\n",
        "\n",
        "        if step % lstm_log_interval == 0 or step == 1:\n",
        "            print(f\"[batch {step:4}/{len(train_loader)}] \"\n",
        "            f\"loss={loss.item():.3f}\")\n",
        "\n",
        "    return total / ntok # token‑average loss per epoch\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Validation – evaluate_loss_attn (no‑grad)\n",
        "# ------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_loss_attn(model_enc, model_dec, loader, criterion, pad_id=PAD_ID):\n",
        "    model_enc.eval()\n",
        "    model_dec.eval()\n",
        "    total, ntok = 0.0, 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        src_mask = (src != pad_id)\n",
        "\n",
        "        enc_out, hidden = model_enc(src)\n",
        "        logits, _ = model_dec(tgt[:, :-1], hidden, enc_out)\n",
        "\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            tgt[:, 1:].reshape(-1)\n",
        "        )\n",
        "        tokens = (tgt[:, 1:] != pad_id).sum().item()\n",
        "        total += loss.item() * tokens\n",
        "        ntok  += tokens\n",
        "    avg = total / ntok\n",
        "    ppl = math.exp(avg)\n",
        "    return avg, ppl\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Epoch loop with plateau scheduler\n",
        "# ------------------------------------------------------------\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "for epoch in range(1, lstm_epochs+1):\n",
        "    loss = train_epoch() # one train pass\n",
        "    val_loss, val_ppl = evaluate_loss_attn(attn_enc, attn_dec, val_loader, criterion) # validation\n",
        "    print(f\"Epoch {epoch}: Train loss={loss:.3f}, Val loss={val_loss:.3f}, Val ppl={val_ppl:.3f}\")\n",
        "\n",
        "    # Reduce LR if no improvement; scheduler looks at val_loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "elapsed = time.perf_counter() - t0\n",
        "print(f\"[LSTM with Attn] Wall-clock training time : {elapsed/60:6.2f} min\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yxhz-g7e9cP"
      },
      "source": [
        "### 6-1. Transformer: Class definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orZahp_tfB_W"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------\n",
        "# PositionalEncoding – sinusoidal schedule explained\n",
        "# ------------------------------------------------------------------\n",
        "# Transformers have no recurrence or convolution, so they need an\n",
        "# explicit signal that tells them token #3 comes after token #2”.  This\n",
        "# positional encoding is added to the token embeddings before the\n",
        "# sequence enters the encoder/decoder.\n",
        "#\n",
        "# We use the classic sinusoidal embedding from the original Vaswani et\n",
        "# al. (2017) paper because:\n",
        "#   • it is fixed (no extra parameters to learn), and\n",
        "#   • any sequence length can be extrapolated thanks to sine/cosine\n",
        "#     periodicity.\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"Pre‑computes a [1, max_len, d_model] tensor of sinusoids.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        d_model: dimensionality of embeddings fed into the Transformer.\n",
        "        max_len: longest sequence the model will ever see at inference.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # ------------------------------------------------------------------\n",
        "        # 1.  Build a lookup table `pe` where row i = position i (0‑indexed)\n",
        "        # ------------------------------------------------------------------\n",
        "        pe = torch.zeros(max_len, d_model) # [T, D]\n",
        "\n",
        "        # Positions: 0, 1, 2, …, T‑1  → shape [T, 1] so broadcasting works\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        # Denominator term 10000^{2k / d_model} implemented via exp/log.\n",
        "        # Only for even indices 0,2,4,…  (cosine will use the same term)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) # 0,2,4,…\n",
        "                            * -(math.log(10000.0) / d_model) # exponent factor\n",
        "                            ) # shape [D/2]\n",
        "\n",
        "        # Apply sin to even dims; cos to odd dims. Broadcasting does the\n",
        "        # heavy lifting so no explicit loops are needed.\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term) # even indices  (0,2,…)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term) # odd  indices  (1,3,…)\n",
        "\n",
        "        # Transformer expects batch dimension first, so unsqueeze(0) → [1,T,D]\n",
        "        # `register_buffer` marks the tensor as part of the module’s state\n",
        "        # (saved with .state_dict()) but not a learnable parameter.\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Add positional encodings to input embeddings.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : [B, T, D] – token embeddings coming from `nn.Embedding`.\n",
        "        Returns\n",
        "        -------\n",
        "        out : [B, T, D] – embeddings plus positional signal.\n",
        "        \"\"\"\n",
        "        # Slice the first T positions (x.size(1)) and rely on broadcasting\n",
        "        # over the batch dimension: [1,T,D] + [B,T,D] → [B,T,D].\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# TransformerModel – step-by-step TODOs\n",
        "# ------------------------------------------------------------------\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab, d_model=256, nhead=4, nlayers=2):\n",
        "        super().__init__()\n",
        "        # -------- TODO students implement below -------- #\n",
        "        # 1. Token embedding with `padding_idx=PAD_ID` and embed_dim = d_model\n",
        "        # 2. PositionalEncoding instance (no learnable params)\n",
        "        # 3. Encoder–decoder stack:\n",
        "        #       enc_layer = nn.TransformerEncoderLayer(d_model, nhead,\n",
        "        #                       dim_feedforward=4*d_model, batch_first=True)\n",
        "        #       self.encoder = nn.TransformerEncoder(enc_layer, nlayers)\n",
        "        #   Repeat similarly for `nn.TransformerDecoder`.  Remember to use\n",
        "        #   batch_first=True so tensors stay [B, T, D].\n",
        "        # 4. Final linear layer maps D → vocab logits.\n",
        "        # ---------------------------------------------- #\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt,\n",
        "                src_key_padding_mask=None,\n",
        "                tgt_key_padding_mask=None,\n",
        "                tgt_mask=None,\n",
        "                memory_key_padding_mask=None):\n",
        "\n",
        "        \"\"\"Forward pass with flexible masking.\n",
        "\n",
        "        * `src_key_padding_mask`    : [B, T_src]  – True where PAD in src\n",
        "        * `tgt_key_padding_mask`    : [B, T_tgt]  – True where PAD in tgt\n",
        "        * `tgt_mask` (causal)       : [T_tgt, T_tgt] – usually `generate_square_subsequent_mask(T_tgt)`\n",
        "        * `memory_key_padding_mask` : masks encoder output; defaults to src mask\n",
        "        \"\"\"\n",
        "        # -------- TODO students implement below -------- #\n",
        "        # 1. Embed + add/get positional encodings:\n",
        "        # 2. Encoder produces `memory` [B, T_src, D]\n",
        "        # 3. Decoder consumes (tgt, memory) and returns hidden states [B, T_tgt, D].\n",
        "        #    Pass all masking arguments to ensure padding & causality.\n",
        "        # 4. Project decoder outputs through `self.fc` → logits [B, T_tgt, vocab]\n",
        "        # 5. Return logits.\n",
        "        # ---------------------------------------------- #\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WubBW5t8frY4"
      },
      "source": [
        "### 6-2. Transformer: Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwYrMTw0u1d9"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Model instantiation\n",
        "# ------------------------------\n",
        "trans_model = TransformerModel(VOCAB, d_model, nhead, nlayers).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pWP7El-fvRC"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# Training loop – Encoder–Decoder Transformer with AdamW\n",
        "# ================================================================\n",
        "# This block is the Transformer counterpart to the LSTM and Attn-LSTM\n",
        "# loops shown earlier.  It introduces two new ingredients:\n",
        "#   • `torch.optim.AdamW`  – Adam variant with decoupled weight decay.\n",
        "#   • A causal mask (`gen_square_sub_mask`) so the decoder can’t peek\n",
        "#     at future tokens during training.\n",
        "\n",
        "# ------------------------------\n",
        "# Optimizer\n",
        "# ------------------------------\n",
        "\n",
        "# AdamW is preferred for Transformers; betas match the original paper.\n",
        "optimizer = torch.optim.AdamW(trans_model.parameters(), lr=trans_base_lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# ------------------------------\n",
        "# Utility: generate causal decoder mask\n",
        "# ------------------------------\n",
        "def gen_square_sub_mask(sz, device):\n",
        "    \"\"\"Upper‑triangular matrix with −inf above the main diagonal.\n",
        "\n",
        "    When added to query–key scores inside `nn.MultiheadAttention`, these\n",
        "    −inf values turn into 0 after softmax → effectively masking future\n",
        "    positions.\n",
        "    \"\"\"\n",
        "    return torch.triu(torch.full((sz, sz), float('-inf'),\n",
        "                      device=device),\n",
        "                      diagonal=1) # start one step above main diagonal\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# train_epoch() for Transformer\n",
        "# ------------------------------------------------------------\n",
        "def train_epoch():\n",
        "    trans_model.train()\n",
        "    total_loss, total_tok = 0.0, 0\n",
        "\n",
        "    for step, (src, tgt) in enumerate(train_loader, 1):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Padding masks (True where PAD)\n",
        "        src_key_padding = src.eq(PAD_ID) # [B, T_src]\n",
        "        tgt_input = tgt[:, :-1] # decoder inputs (shifted)\n",
        "        tgt_key_padding = tgt_input.eq(PAD_ID) # [B, T_tgt]\n",
        "\n",
        "        # Causal mask – ensures tokens attend only to earlier positions.\n",
        "        tgt_mask = gen_square_sub_mask(tgt_input.size(1), device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = trans_model(src, tgt_input,\n",
        "                       src_key_padding_mask=src_key_padding,\n",
        "                       tgt_key_padding_mask=tgt_key_padding,\n",
        "                       tgt_mask=tgt_mask) # [B, T_tgt, V]\n",
        "\n",
        "        # Label‑smoothed loss (defined earlier) expects log‑probs\n",
        "        loss = label_smoothed_nll_loss(F.log_softmax(logits, -1), # convert to log‑p\n",
        "                                       tgt[:, 1:]) # decoder targets (shifted)\n",
        "\n",
        "        # Back‑prop and optimization\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(trans_model.parameters(), 1.0)\n",
        "        optimizer.step();\n",
        "\n",
        "        # Aggregate stats\n",
        "        ntok = tgt[:, 1:].ne(PAD_ID).sum().item()\n",
        "        total_loss += loss.item() * ntok\n",
        "        total_tok  += ntok\n",
        "\n",
        "        if step % trans_log_interval == 0 or step == 1:\n",
        "            print(f\"[batch {step:4}/{len(train_loader)}] \"\n",
        "            f\"loss={loss.item():.3f}\")\n",
        "\n",
        "    return total_loss / total_tok # token‑average loss over epoch\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Validation loop (no‑grad)\n",
        "# ------------------------------------------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_loss_transformer(model, loader, criterion, pad_id=PAD_ID):\n",
        "    model.eval()\n",
        "    total, ntok = 0.0, 0\n",
        "\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        src_key_padding = src.eq(pad_id)\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_key_padding = tgt_input.eq(pad_id)\n",
        "        tgt_mask = gen_square_sub_mask(tgt_input.size(1), device)\n",
        "\n",
        "        logits = model(src, tgt_input,\n",
        "                     src_key_padding_mask=src_key_padding,\n",
        "                     tgt_key_padding_mask=tgt_key_padding,\n",
        "                     tgt_mask=tgt_mask)\n",
        "\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)),\n",
        "                       tgt[:, 1:].reshape(-1))\n",
        "\n",
        "        tokens = (tgt[:, 1:] != pad_id).sum().item()\n",
        "        total += loss.item() * tokens\n",
        "        ntok += tokens\n",
        "\n",
        "    avg = total / ntok\n",
        "    ppl = math.exp(avg)\n",
        "    return avg, ppl\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Epoch loop\n",
        "# ------------------------------------------------------------\n",
        "t0 = time.perf_counter()\n",
        "\n",
        "for epoch in range(1, trans_epochs + 1):\n",
        "    loss = train_epoch()\n",
        "    val_loss, val_ppl = evaluate_loss_transformer(trans_model, val_loader, criterion)\n",
        "    print(f\"Epoch {epoch}: Train loss={loss:.3f}, Val loss={val_loss:.3f}, Val ppl={val_ppl:.3f}\")\n",
        "\n",
        "elapsed = time.perf_counter() - t0\n",
        "print(f\"[Transformer] Wall-clock training time : {elapsed/60:6.2f} min\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3rplS1ujao0"
      },
      "source": [
        "### 7. Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRANP8Iajkge"
      },
      "outputs": [],
      "source": [
        "# Example sentence\n",
        "en_sentence = \"A person is wearing a hat.\"\n",
        "\n",
        "# You may not get good translation results since we've trained the models\n",
        "# with small capacity and datasets.\n",
        "# However, you can see that the more advanced model can capture some contexts.\n",
        "# You are totally allowed to change the sentence for different tries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5mJ5XN5j67M"
      },
      "source": [
        "### 7-1. Tranlation example: LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grJiIpCkkASF"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# Greedy inference – `translate_lstm`\n",
        "# ================================================================\n",
        "# At training time we ran teacher forcing (feeding gold tokens).\n",
        "# During inference we must generate one token at a time and feed each\n",
        "# prediction back into the decoder. The helper below performs greedy\n",
        "# decoding—always picking the highest‐probability token at every step.\n",
        "\n",
        "def translate_lstm(src_sentence):\n",
        "    \"\"\"Translate English→French using the trained LSTM seq2seq model.\n",
        "\n",
        "    Steps\n",
        "    -----\n",
        "    1.  Switch encoder/decoder to `eval()` so dropout is disabled.\n",
        "    2.  `encode()` the raw source sentence → list[int].  Wrap in a\n",
        "        batch‐dim `[1, T]` and move to device.\n",
        "    3.  Run the encoder once to get all hidden states + final (h, c).\n",
        "    4.  Initialize the decoder input with just `<s>` (BOS).\n",
        "    5.  Loop up to `max_len` – each iteration:\n",
        "        a. Feed the last generated token to decoder.\n",
        "        b. Take `argmax` over vocabulary to get the next token ID.\n",
        "        c. Append the new token to `ys`.\n",
        "        d. If the token is `</s>` (EOS) → break early.\n",
        "    6.  Remove BOS/EOS, convert IDs back to text with `sp.decode()`.\n",
        "    \"\"\"\n",
        "    lstm_enc.eval() # 1. evaluation mode\n",
        "    lstm_dec.eval()\n",
        "\n",
        "    src_ids = torch.tensor([encode(src_sentence)], device=device) # 2.\n",
        "\n",
        "    enc_out, hidden = lstm_enc(src_ids) # 3. encoder forward\n",
        "\n",
        "    ys = torch.tensor([[BOS_ID]], device=device) # 4. start symbol\n",
        "\n",
        "    for _ in range(max_len): # 5. generate token by token\n",
        "          logits, hidden = lstm_dec(ys[:, -1:], hidden) # a. last token only\n",
        "          next_id = logits[:, -1, :].argmax(-1) # b. greedy pick\n",
        "          ys = torch.cat([ys, next_id.unsqueeze(1)], dim=1) # c. append\n",
        "          if next_id.item() == EOS_ID: # d. stop condition\n",
        "              break\n",
        "\n",
        "    tgt_tokens = ys[0, 1:-1].tolist() # strip BOS & EOS\n",
        "    return sp.decode(tgt_tokens) # 6. detokenize\n",
        "\n",
        "print(\"LSTM Translation Result:\")\n",
        "print(f\"▶︎ {en_sentence}\")\n",
        "print(\"   →\", translate_lstm(en_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFpsq3wrlz7c"
      },
      "source": [
        "### 7-2. Tranlation example: LSTM with Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWlEvDB8l3fM"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# Greedy inference – `translate_attn`\n",
        "# ================================================================\n",
        "\n",
        "def translate_attn(src_sentence):\n",
        "    attn_enc.eval()\n",
        "    attn_dec.eval()\n",
        "\n",
        "    src_ids = torch.tensor([encode(src_sentence)], device=device)\n",
        "\n",
        "    enc_out, hidden = attn_enc(src_ids)\n",
        "\n",
        "    ys = torch.tensor([[BOS_ID]], device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "          logits, hidden = attn_dec(ys[:, -1:], hidden, enc_out)\n",
        "          next_id = logits[:, -1, :].argmax(-1)\n",
        "          ys = torch.cat([ys, next_id.unsqueeze(1)], dim=1)\n",
        "          if next_id.item() == EOS_ID:\n",
        "              break\n",
        "\n",
        "    tgt_tokens = ys[0, 1:-1].tolist()\n",
        "    return sp.decode(tgt_tokens)\n",
        "\n",
        "print(\"LSTM with Attention Translation Result:\")\n",
        "print(f\"▶︎ {en_sentence}\")\n",
        "print(\"   →\", translate_attn(en_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXw26GqImFrP"
      },
      "source": [
        "### 7-3. Tranlation example: Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p2owXkTmIUw"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# 16.  Greedy inference – `translate_trans` for Transformer\n",
        "# ================================================================\n",
        "# This routine mirrors `translate_lstm` but uses the Transformer model.\n",
        "# Main differences:\n",
        "#   • We precompute the memory (encoder output) once.\n",
        "#   • Every decoding step requires a causal mask for self‑attention.\n",
        "#   • Padding masks must be passed to both decoder and encoder–decoder\n",
        "#     attention so PAD tokens don't influence the context.\n",
        "\n",
        "def translate_trans(src_sent):\n",
        "    \"\"\"Translate a single sentence with the trained Transformer.\"\"\"\n",
        "    trans_model.eval() # disable dropout\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 1.  Encode the source sentence ONCE\n",
        "    # ------------------------------------------------------------\n",
        "    src_ids = torch.tensor([encode(src_sent)], device=device) # [1, T_src]\n",
        "    src_key = src_ids.eq(PAD_ID) # [1, T_src] bool\n",
        "\n",
        "    # token embed + positional encodings\n",
        "    memory = trans_model.embed(src_ids) # [1, T_src, D]\n",
        "    memory = trans_model.pos(memory) # add sin/cos positions\n",
        "\n",
        "    # run through the encoder stack → `memory`\n",
        "    memory = trans_model.encoder(memory, src_key_padding_mask=src_key)\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 2.  Autoregressive decoder loop (greedy)\n",
        "    # ------------------------------------------------------------\n",
        "    ys = torch.tensor([[BOS_ID]], device=device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Causal mask grows with sequence length\n",
        "        tgt_mask = gen_square_sub_mask(ys.size(1), device) # [T_tgt, T_tgt]\n",
        "        tgt_key  = ys.eq(PAD_ID) # padding mask\n",
        "\n",
        "         # Embed + positional\n",
        "        out = trans_model.embed(ys)\n",
        "        out = trans_model.pos(out)\n",
        "\n",
        "        # Decoder: queries = out, keys/values from memory\n",
        "        out = trans_model.decoder(out, memory,\n",
        "                                  tgt_mask=tgt_mask,\n",
        "                                  tgt_key_padding_mask=tgt_key,\n",
        "                                  memory_key_padding_mask=src_key\n",
        "                                  ) # [1, T_tgt, D]\n",
        "\n",
        "        # Project newest time‑step to vocabulary and pick argmax\n",
        "        next_tok = trans_model.fc(out[:, -1, :]).argmax(-1) # [1]\n",
        "\n",
        "        ys = torch.cat([ys, next_tok.unsqueeze(1)], dim=1) # append\n",
        "        if next_tok.item() == EOS_ID: break # stop if </s> generated\n",
        "\n",
        "    # Strip BOS/EOS and detokenize\n",
        "    return sp.decode(ys[0, 1:-1].tolist())\n",
        "\n",
        "print(\"Transformer Translation Result:\")\n",
        "print(f\"▶︎ {en_sentence}\")\n",
        "print(\"   →\", translate_trans(en_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ0h2_H1M32Z"
      },
      "source": [
        "### 8. Model summary\n",
        "Shows number of parameters, multiply-adds (MACs) of the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPPV_CgVpJ4v"
      },
      "outputs": [],
      "source": [
        "# Summary of LSTM ENC\n",
        "print(summary(lstm_enc, input_size=(1, 30), dtypes=[torch.long], col_names=(\"num_params\", \"mult_adds\")))\n",
        "\n",
        "# Summary of LSTM DEC\n",
        "y = torch.randint(0, VOCAB, (1, 29), dtype=torch.long).to(device)\n",
        "h = torch.randn(lstm_layers, 1, lstm_hidden).to(device)\n",
        "c = torch.randn(lstm_layers, 1, lstm_hidden).to(device)\n",
        "hidden = (h, c)\n",
        "print(summary(lstm_dec, input_data=(y, hidden), dtypes=[torch.long], col_names=(\"num_params\", \"mult_adds\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6QcdtBwspyh"
      },
      "outputs": [],
      "source": [
        "# Summary of LSTM w/ ATTN ENC\n",
        "print(summary(attn_enc, input_size=(1, 30), dtypes=[torch.long], col_names=(\"num_params\", \"mult_adds\")))\n",
        "\n",
        "# Summary of LSTM w/ ATTN DEC\n",
        "y = torch.randint(0, VOCAB, (1, 29), dtype=torch.long).to(device)\n",
        "h = torch.randn(lstm_layers, 1, lstm_hidden).to(device)\n",
        "c = torch.randn(lstm_layers, 1, lstm_hidden).to(device)\n",
        "hidden = (h, c)\n",
        "enc_outputs = torch.randn(1, 29, lstm_hidden).to(device)\n",
        "print(summary(attn_dec, input_data=(y, hidden, enc_outputs), dtypes=[torch.long], col_names=(\"num_params\", \"mult_adds\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwSfg-7KvA-d"
      },
      "outputs": [],
      "source": [
        "# Summary of Transformer\n",
        "print(summary(trans_model, input_size=[(1, 30), (1, 29)], dtypes=[torch.long, torch.long], col_names=(\"num_params\", \"mult_adds\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZy1zUOrJbFm"
      },
      "source": [
        "### Discussion\n",
        "1.   Compare the final validation losses for three models and provide an explanation of the difference.\n",
        "2.   Discuss which model is the most efficient in terms of computational complexity and translation performance. Give a reason why.\n",
        "3.   Discuss which model is the most efficient in terms of model size and translation performance. Give a reason why.\n",
        "4.   Discuss which model is the most efficient in terms of training time and translation performance. Give a reason why.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
